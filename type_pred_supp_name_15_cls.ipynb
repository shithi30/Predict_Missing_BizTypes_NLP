{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26256,
     "status": "ok",
     "timestamp": 1667892249768,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "pdrZ8gRDATiY",
    "outputId": "27d39ca5-4cbc-45a4-ddfc-8d18d284a046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1913,
     "status": "ok",
     "timestamp": 1667892251677,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "YOACUCY2Ar1I",
    "outputId": "873414c1-21fb-4cac-b43f-eab51ec83af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line (header) looks like this:\n",
      "\n",
      "\"category\",\"text\"\n",
      "\n",
      "Each data point looks like this:\n",
      "\n",
      "grocery,আলমগির\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"/content/drive/MyDrive/Colab Notebooks/type_pred_supp_name_15_cls.csv\", 'r') as csvfile:\n",
    "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
    "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667892251677,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "timau0NaBu-F"
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = 500000\n",
    "EMBEDDING_DIM = 16\n",
    "MAXLEN = 20\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "TRAINING_SPLIT = .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1667892251678,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "Luvsg8MCDMZk"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    Removes a list of stopwords\n",
    "    \n",
    "    Args:\n",
    "        sentence (string): sentence to remove the stopwords from\n",
    "    \n",
    "    Returns:\n",
    "        sentence (string): lowercase sentence without the stopwords\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def parse_data_from_file(filename):\n",
    "    \"\"\"\n",
    "    Extracts sentences and labels from a CSV file\n",
    "    \n",
    "    Args:\n",
    "        filename (string): path to the CSV file\n",
    "    \n",
    "    Returns:\n",
    "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            labels.append(row[0])\n",
    "            sentence = row[1]\n",
    "            sentence = remove_stopwords(sentence)\n",
    "            sentences.append(sentence)\n",
    "\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1426,
     "status": "ok",
     "timestamp": 1667892253100,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "cgQlzBoUKxe3",
    "outputId": "e42faec7-975b-432b-ac2c-14dabb185dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 49000 sentences in the dataset.\n",
      "\n",
      "First sentence has 1 words (after removing stopwords).\n",
      "\n",
      "There are 49000 labels in the dataset.\n",
      "\n",
      "The first 5 labels are ['grocery', 'distributororwholesale', 'bakeryandconfectionery', 'electronics', 'mfsmobilerecharge']\n"
     ]
    }
   ],
   "source": [
    "# Test the functions\n",
    "sentences, labels = parse_data_from_file(\"/content/drive/MyDrive/Colab Notebooks/type_pred_supp_name_15_cls.csv\")\n",
    "\n",
    "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
    "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
    "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
    "print(f\"The first 5 labels are {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1667892253101,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "kVxABLx4K0J8"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTIONS: train_val_split\n",
    "def train_val_split(sentences, labels, training_split):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of string): lower-cased sentences without stopwords\n",
    "        labels (list of string): list of labels\n",
    "        training split (float): proportion of the dataset to convert to include in the train set\n",
    "    \n",
    "    Returns:\n",
    "        train_sentences, validation_sentences, train_labels, validation_labels - lists containing the data splits\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Compute the number of sentences that will be used for training (should be an integer)\n",
    "    train_size = round(len(sentences)*training_split)\n",
    "\n",
    "    # Split the sentences and labels into train/validation splits\n",
    "    train_sentences = sentences[0:train_size]\n",
    "    train_labels = labels[0:train_size]\n",
    "\n",
    "    validation_sentences = sentences[train_size:]\n",
    "    validation_labels = labels[train_size:]\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return train_sentences, validation_sentences, train_labels, validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1667892253102,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "gxFDh94CLHeX",
    "outputId": "12a38f48-4c6e-4580-f824-96438adf852d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41650 sentences for training.\n",
      "\n",
      "There are 41650 labels for training.\n",
      "\n",
      "There are 7350 sentences for validation.\n",
      "\n",
      "There are 7350 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1667892253103,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "vNsHuVQrLLO4"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: fit_tokenizer\n",
    "def fit_tokenizer(train_sentences, num_words, oov_token):\n",
    "    \"\"\"\n",
    "    Instantiates the Tokenizer class on the training sentences\n",
    "    \n",
    "    Args:\n",
    "        train_sentences (list of string): lower-cased sentences without stopwords to be used for training\n",
    "        num_words (int) - number of words to keep when tokenizing\n",
    "        oov_token (string) - symbol for the out-of-vocabulary token\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Instantiate the Tokenizer class, passing in the correct values for num_words and oov_token\n",
    "    tokenizer = Tokenizer(num_words = num_words, oov_token=oov_token)\n",
    "    \n",
    "    # Fit the tokenizer to the training sentences\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1348,
     "status": "ok",
     "timestamp": 1667892254443,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "VNkKm1OJLRbS",
    "outputId": "e504f4ee-03fb-4a2c-b0f4-454648ba65a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 70258 words\n",
      "\n",
      "<OOV> token included in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "tokenizer = fit_tokenizer(train_sentences, NUM_WORDS, OOV_TOKEN)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1667892254443,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "RTm_H0hqLUpG"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: seq_and_pad\n",
    "def seq_and_pad(sentences, tokenizer, padding, maxlen):\n",
    "    \"\"\"\n",
    "    Generates an array of token sequences and pads them to the same length\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of string): list of sentences to tokenize and pad\n",
    "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
    "        padding (string): type of padding to use\n",
    "        maxlen (int): maximum length of the token sequence\n",
    "    \n",
    "    Returns:\n",
    "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE\n",
    "       \n",
    "    # Convert sentences to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # Pad the sequences using the correct padding and maxlen\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1466,
     "status": "ok",
     "timestamp": 1667892255907,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "Yzkxe3JXLZTc",
    "outputId": "fb831c58-54ed-4162-b544-b5ca03e62db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded training sequences have shape: (41650, 20)\n",
      "\n",
      "Padded validation sequences have shape: (7350, 20)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_padded_seq = seq_and_pad(train_sentences, tokenizer, PADDING, MAXLEN)\n",
    "val_padded_seq = seq_and_pad(val_sentences, tokenizer, PADDING, MAXLEN)\n",
    "\n",
    "print(f\"Padded training sequences have shape: {train_padded_seq.shape}\\n\")\n",
    "print(f\"Padded validation sequences have shape: {val_padded_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1667892255908,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "5ipm6TUVpBji"
   },
   "outputs": [],
   "source": [
    "# categories into numerics\n",
    "categories=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667892255908,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "8LLCZawvLdki"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: tokenize_labels\n",
    "def tokenize_labels(all_labels, split_labels):\n",
    "    \"\"\"\n",
    "    Tokenizes the labels\n",
    "    \n",
    "    Args:\n",
    "        all_labels (list of string): labels to generate the word-index from\n",
    "        split_labels (list of string): labels to tokenize\n",
    "    \n",
    "    Returns:\n",
    "        label_seq_np (array of int): tokenized labels\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Instantiate the Tokenizer (no additional arguments needed)\n",
    "    label_tokenizer = Tokenizer()\n",
    "    \n",
    "    # Fit the tokenizer on all the labels\n",
    "    label_tokenizer.fit_on_texts(all_labels)\n",
    "\n",
    "    global categories\n",
    "    categories = list(label_tokenizer.word_index)\n",
    "    \n",
    "    # Convert labels to sequences\n",
    "    label_seq = label_tokenizer.texts_to_sequences(split_labels) \n",
    "    \n",
    "    # Convert sequences to a numpy array. Don't forget to substact 1 from every entry in the array!\n",
    "    label_seq_np = np.array(label_seq)-1\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return label_seq_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1667,
     "status": "ok",
     "timestamp": 1667892257572,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "by_vFuCaLkUW",
    "outputId": "f50b3719-6ac9-4133-c906-de4107cb32ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels are:  ['grocery', 'distributororwholesale', 'mfsmobilerecharge', 'pharmacy', 'electronics', 'clothstore', 'hardware', 'agriculture', 'bakeryandconfectionery', 'motorrepair', 'stationery', 'cosmetics', 'householdandfurniture', 'tailors', 'shoestore']\n",
      "First 5 labels of the training set should look like this:\n",
      "[[0]\n",
      " [1]\n",
      " [8]\n",
      " [4]\n",
      " [2]]\n",
      "\n",
      "First 5 labels of the validation set should look like this:\n",
      "[[5]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [0]]\n",
      "\n",
      "Tokenized labels of the training set have shape: (41650, 1)\n",
      "\n",
      "Tokenized labels of the validation set have shape: (7350, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_label_seq = tokenize_labels(labels, train_labels)\n",
    "val_label_seq = tokenize_labels(labels, val_labels)\n",
    "\n",
    "print(\"Labels are: \", categories)\n",
    "\n",
    "print(f\"First 5 labels of the training set should look like this:\\n{train_label_seq[:5]}\\n\")\n",
    "print(f\"First 5 labels of the validation set should look like this:\\n{val_label_seq[:5]}\\n\")\n",
    "print(f\"Tokenized labels of the training set have shape: {train_label_seq.shape}\\n\")\n",
    "print(f\"Tokenized labels of the validation set have shape: {val_label_seq.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1667892257573,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "rkaYz4axLo2X"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_model\n",
    "def create_model(num_words, embedding_dim, maxlen):\n",
    "    \"\"\"\n",
    "    Creates a text classifier model\n",
    "    \n",
    "    Args:\n",
    "        num_words (int): size of the vocabulary for the Embedding layer input\n",
    "        embedding_dim (int): dimensionality of the Embedding layer output\n",
    "        maxlen (int): length of the input sequences\n",
    "    \n",
    "    Returns:\n",
    "        model (tf.keras Model): the text classifier model\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.random.set_seed(123)\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(num_words, embedding_dim, input_length=maxlen),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(15, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']) \n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 551235,
     "status": "ok",
     "timestamp": 1667892808788,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "brXB7uSATs6j",
    "outputId": "f06b77e5-6c7d-441d-a103-6ca03b0c537c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1302/1302 [==============================] - 113s 86ms/step - loss: 2.2930 - accuracy: 0.2272 - val_loss: 2.1700 - val_accuracy: 0.2659\n",
      "Epoch 2/5\n",
      "1302/1302 [==============================] - 109s 83ms/step - loss: 2.0435 - accuracy: 0.3228 - val_loss: 2.0688 - val_accuracy: 0.3184\n",
      "Epoch 3/5\n",
      "1302/1302 [==============================] - 109s 84ms/step - loss: 1.8507 - accuracy: 0.4161 - val_loss: 2.0406 - val_accuracy: 0.3359\n",
      "Epoch 4/5\n",
      "1302/1302 [==============================] - 110s 85ms/step - loss: 1.6685 - accuracy: 0.4881 - val_loss: 2.0755 - val_accuracy: 0.3433\n",
      "Epoch 5/5\n",
      "1302/1302 [==============================] - 109s 84ms/step - loss: 1.4899 - accuracy: 0.5476 - val_loss: 2.1469 - val_accuracy: 0.3412\n"
     ]
    }
   ],
   "source": [
    "model = create_model(NUM_WORDS, EMBEDDING_DIM, MAXLEN)\n",
    "\n",
    "history = model.fit(train_padded_seq, train_label_seq, epochs=5, validation_data=(val_padded_seq, val_label_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1667892808789,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "zY0KvbNsUL9Y"
   },
   "outputs": [],
   "source": [
    "# def plot_graphs(history, metric):\n",
    "#     plt.plot(history.history[metric])\n",
    "#     plt.plot(history.history[f'val_{metric}'])\n",
    "#     plt.xlabel(\"Epochs\")\n",
    "#     plt.ylabel(metric)\n",
    "#     plt.legend([metric, f'val_{metric}'])\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_graphs(history, \"accuracy\")\n",
    "# plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1667892809674,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "uxIWu8SArjum",
    "outputId": "f8af53f1-5d68-47df-a980-0f652c91d01b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "প্রসাধ মুড়া: grocery\n",
      "itel sojit jom jom rasil mi abbus b raj tani masik modi onnano bazar r r romi b sumon b kabar walton ismail: electronics\n",
      "জাহানশাহ হার্ডওয়ার বলির হাট মালেক এন্ড সোন্স খাজা রোড খরম পাড়া মুনিরা হার্ডওয়ার বলির হাট: hardware\n",
      "monir dhokan: grocery\n",
      "cocola group b mukhless b group of olympic nurol munshi চা পাতা সাপ্লায়ার সাপ্লায়ার অফ প্রাণ রোবো: grocery\n",
      "লতিফ ভাই ঝিনা: grocery\n",
      "abu hanif: mfsmobilerecharge\n",
      "nepa saly rafsor আকরাম ভাই বৌ পলাশিয়া মেসার্স সালমান বস্ত্রালয় selem vi হাবিল উদ্দিন কান্দাপাড়া: clothstore\n",
      "tushar vai: mfsmobilerecharge\n",
      "prince plastic: distributororwholesale\n",
      "মার্শাল এগ্রভেট সুইট এগ্রোভেট হাবিব ট্রেডার্স: clothstore\n",
      "rasel: grocery\n",
      "পাথ: grocery\n",
      "আলোম মেসিনা: grocery\n",
      "syed mia: mfsmobilerecharge\n",
      "ankush gp lood mobail compani: mfsmobilerecharge\n",
      "তাহমিনা আক্তার: mfsmobilerecharge\n",
      "incepta pharmaceutical jmi delowar mamun navana: pharmacy\n",
      "my gp: grocery\n",
      "hossain kasem feni mamun v murgi morshed opsoni rony: distributororwholesale\n",
      "kamrul surap: distributororwholesale\n",
      "abu parves rubel cha sahalom: mfsmobilerecharge\n",
      "dulal: grocery\n",
      "topon sen vai: mfsmobilerecharge\n",
      "ছামিউল দারিয়াপুর নুরু মিয়া মহাজন সাইফুল ইসলাম dst হাসেম ভাই: distributororwholesale\n",
      "টাটা: distributororwholesale\n",
      "sahadad ma: distributororwholesale\n",
      "kuddus milon selun: distributororwholesale\n",
      "jahed green tech rana vai: distributororwholesale\n",
      "সানোয়ার মামা: mfsmobilerecharge\n",
      "তুহিনের মা: distributororwholesale\n",
      "anika enterprise dhaka: distributororwholesale\n",
      "abrar packging cradit card dhiya dbbl ibrahim enterprise ilma packging pundra bardhan printing sordar plastic s tel: distributororwholesale\n",
      "rinku kaka জমা: grocery\n",
      "durjoy dhaka: distributororwholesale\n",
      "battry: distributororwholesale\n",
      "baba bay nij billal milk এনামুল বাই মাজাহারুল বাই: grocery\n",
      "নিউ আনন্দ হিমেল স্টোর: grocery\n",
      "md mahabub: mfsmobilerecharge\n",
      "jafor food: grocery\n",
      "md yousuf raha: mfsmobilerecharge\n",
      "abu said bai r bazar robi anam enterprise r bazar rasal bai bohardarhat lollipop s s markating r bazar: distributororwholesale\n",
      "hannan ibn sina natural lotfor leon pharma md ataher alli orion pharma md mehedi hassan ibnsina pharma md siddiqur rahman opsonin oaliulla benham silva pharmaceutical z alam medic: pharmacy\n",
      "asera beti: grocery\n",
      "আশরাফুলr: grocery\n",
      "modern library: pharmacy\n",
      "শবিকুল ভাই বক্তার পুর: distributororwholesale\n",
      "johurul s r telecom roommate shihab bhai: distributororwholesale\n",
      "আমিরন রকেল দোক ন ইসলাম ট্রেডিং কুখি জিলানি স্টোর: distributororwholesale\n",
      "জামান স্টোর: grocery\n",
      "জামাল দাউদকান্দি নাজমুল এন্টারপ্রাইজ বিসমিল্লাহ ট্রেডার্স সৌদিয়া ট্রেডার্স: hardware\n",
      "mostafij sohel আজমির খানাডুলি টপ চয়েচ বিসমিল্লাহ আকবর শহিদ সুইং টংঙ্গী শাহানাজ বাউ টাকা মাসিক শাহানাজ বাউনিয়া: distributororwholesale\n",
      "প্রিতম: distributororwholesale\n",
      "bengal homoeo hall deeplaid laboratories ltd এস ফার্মা লিমিটেড: pharmacy\n",
      "ওনার: grocery\n",
      "omera lpg: distributororwholesale\n",
      "alamin: mfsmobilerecharge\n",
      "renata com: pharmacy\n",
      "asraful f robo: distributororwholesale\n",
      "shamim danish lion: grocery\n",
      "habib: mfsmobilerecharge\n",
      "মো আসাদুল নেসলে মেগি মো সাইফুল ভাই মো সালাম হোসেন মো হাফিজ সিপ্রীড: grocery\n",
      "islam ruma sendel আবীর বার্মিজ পয়েন্ট এইচ কে বার্মিজ করিম এম বি জি ক্ক্সবাজার চট্টগ্রাম বিভিন্ন পায়কারী মাল জিলানী বার্মিজ ষ্টোর নোয়েল হোম টেক্সটাইল হেলাল বার্মিজ ষ্টোর: clothstore\n",
      "ovi book আমিন বুক সৌসাইটি আরাফাত লাইব্রেরী আল মারুফ পাবলিকেশন্স আন্দরকিল্লা এশিয়া পেপার এন্ড স্টেশনারী গ্রেটসৌর্স স্টেশনারী ছাত্র বন্ধু লাইব্রেরী আন্দরকিল্লাহ পপুলার প্রোডাক্ট যুবরাজ আন্দরকিল্লা পরিচয় প্রকাশন খ বাংলাবাজার পেপার লাইন আন্দরকিল্লা ফেমাস লাইব্রেরী ফ্রেশ পেপার বসুন্ধরা পেপার বুরো বাংলাদেশ বড়পুল মডার্ণ লাইব্রেরী আন্দরকিল্লা মিজান পাবলিশার্স বাংলাবাজার মেটাডোর প্যান রানা লাইব্রেরী শান্তিবাগ স্টেশনারী শাহ আমানত ট্রেডিং হেলাল থ্রাপিষ্ট: hardware\n",
      "asa delwar shakti dish bill rafique konok plus pabe কর্মসংস্থান ব্যাংক মার্কেন্টাইল ব্যাংক: electronics\n",
      "sojib vi bokar nubabpur: grocery\n",
      "hachan holdi কামাল আলু কালাম চাউল চানাচুর আ মাহবুব কাকা: grocery\n",
      "জিল্লু তালুর বউ ফরিদ বাব রিদয় লতিফ মহোজন শওকত শফিক সামিন সিবলু: grocery\n",
      "nojrul mama gorubad: grocery\n",
      "b colle confidence dolon caca jafor mask joyanta mama mitu vi mask rajsahi t vaista mask raju vi channa m saifu konabari m vena s mostk আবুল মোল্লা আরিফুল ইসলাম আরিফ নাজমা সাটিং মাস্ক: distributororwholesale\n",
      "majo vsi: distributororwholesale\n",
      "liton unilevar ludus od sampo: distributororwholesale\n",
      "আগচারান দারিদ্র সেবা সংগঠন আবু সাঈদ আব্দুল কালাম আলেক মিয়া আসাদুর রহমান সামাদ মিঠু খান সাব্বির রহমান: clothstore\n",
      "নং রুম সজীব: distributororwholesale\n",
      "আলামিন সাইদ মামু: grocery\n",
      "anik robi sr bl esr gp sr mamun আনোয়ার মোহাম্মদ কাকা সবুজ: mfsmobilerecharge\n",
      "amma bashar hanif: mfsmobilerecharge\n",
      " emon taka hoichoi khroch offer income ott income আয় ব্যায় কোচিং: mfsmobilerecharge\n",
      "ফলের আইটেম: grocery\n",
      "m s safia enterprise: distributororwholesale\n",
      "আমার বাড়ি আমার খামার: mfsmobilerecharge\n",
      "buyaw camecal house rent and electricity bill mamun fabrics mr jual lavel man rifat emboidary sanaullah embotary: distributororwholesale\n",
      "airtel sr sawon vai gp sr faruk vai rajib vai gp robi sr utsob vai simanto gp teletalk sr rahman vai: mfsmobilerecharge\n",
      "আক্কাস ভাই আপা খাজা বান্ডার পাহাড়তলী খাজা ষ্টোর ঢাকা ব্যাংক কিস্তি টাকা মনির ভাই সুমন ভাই হক রাইস পাহাডতলী: grocery\n",
      "nizam argon welder abl: pharmacy\n",
      "j জয়দেব দাদা পান t তারেক ভাই: grocery\n",
      "fardin traders hazi traders modhu traders: distributororwholesale\n",
      "atikur oph: distributororwholesale\n",
      "stকলেজ রোড দাইমুল: grocery\n",
      "taslima apu আতিক ভাই কাইয়ুম ভাই দোকান কাজী সাকিব কিস্তি ছোটরা কিস্তি জুলেখা আপু পারভীন আপা মিজান ভাই শ্যামল ভাই সুমন: mfsmobilerecharge\n",
      "chemest vet ibn sina ibn sina her tecno: pharmacy\n",
      "anjon babu helal bhai new: distributororwholesale\n",
      "rasel saddam: grocery\n",
      "rezoan shop: grocery\n",
      "afroza apu fufa mou ratul furniture rishad safin আকিব খান এন্ড সন্স জহিরুল আংকেল দাদু মনি মুন মা রইচ ফুপা রাইছা সাইম মা: distributororwholesale\n",
      "মনেক্কা ব: grocery\n",
      "মোখলেস মাম: distributororwholesale\n",
      "rajib: grocery\n",
      "melon dolavi shapur shamim dolavi cotcatpur: distributororwholesale\n",
      "jahangir dokan rfl canteen ডেইলি সোপিং: distributororwholesale\n",
      "\n",
      "[False  True  True False  True False False  True False False False False\n",
      " False False  True  True  True  True False  True False  True False False\n",
      " False False False False False False False False  True False False False\n",
      " False  True False  True False  True  True  True False False False False\n",
      " False False  True False False  True False  True False  True False  True\n",
      " False  True  True False False False  True  True False  True  True False\n",
      " False False False  True False  True  True False  True False  True  True\n",
      " False  True  True False False False  True False  True False False  True\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "# probabilies against all classes for selective validation data\n",
    "pred_probabilities=model.predict(val_padded_seq[0:100, :])\n",
    "\n",
    "# predictions against selective validation data\n",
    "pred_classes=[categories[item] for item in np.argmax(pred_probabilities, axis=1)]\n",
    "val_shop_names=val_sentences[0:100]\n",
    "\n",
    "for i in range(0, len(val_shop_names)): \n",
    "  print(val_shop_names[i]+ \": \"+pred_classes[i])\n",
    "\n",
    "print()\n",
    "\n",
    "# if predictions match\n",
    "print(np.argmax(pred_probabilities, axis=1)==(np.squeeze(val_label_seq[0:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1594,
     "status": "ok",
     "timestamp": 1667898256685,
     "user": {
      "displayName": "Shithi Maitra",
      "userId": "01287562705842979191"
     },
     "user_tz": -360
    },
    "id": "oOzSKagLFPz8",
    "outputId": "3356ff4a-13cd-446a-a17a-c64e46e73f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# save model and architecture to single file\n",
    "model.save(\"/content/drive/MyDrive/Colab Notebooks/suppname_model_15_cls.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDuLNiHLKOIo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
